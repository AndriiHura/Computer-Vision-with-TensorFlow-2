{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _(x):\n",
    "    print(x)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron(object):\n",
    "    \"\"\"A simple feed-forward artificial neuron.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values.\n",
    "        activation_fn (callable): The activation function.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (float): The bias value, added to the weighted sum.\n",
    "        activation_fn (callable): The activation function.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_inputs, activation_fn):\n",
    "        super().__init__()\n",
    "        # Randomly initializing the weight vector and bias value:\n",
    "        self.W = np.random.rand(num_inputs)\n",
    "        self.b = np.random.rand(1)\n",
    "        self.activation_fn = activation_fn\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward the input signal through the neuron.\"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        return self.activation_fn(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.37454012 0.95071431 0.73199394]]\n",
      "\n",
      "[0.59865848 0.15601864 0.15599452]\n",
      "\n",
      "[0.05808361]\n",
      "\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fixing the random number generator's seed, for reproducible results:\n",
    "np.random.seed(42)\n",
    "# Random input column array of 3 values (shape = `(1, 3)`)\n",
    "x = np.random.rand(3).reshape(1, 3)\n",
    "_(x)\n",
    "# > [[0.37454012 0.95071431 0.73199394]]\n",
    "# Instantiating a Perceptron (simple neuron with step function):\n",
    "step_fn = lambda y: 0 if y <= 0 else 1\n",
    "perceptron = Neuron(num_inputs=x.size, activation_fn=step_fn)\n",
    "_(perceptron.W)\n",
    "_(perceptron.b)\n",
    "out = perceptron.forward(x)\n",
    "_(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    \"\"\"A simple fully-connected NN layer.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size/number of input values.\n",
    "        layer_size (int): The output vector size/number of neurons.\n",
    "        activation_fn (callable): The activation function for this layer.\n",
    "    Attributes:\n",
    "        W (ndarray): The weight values for each input.\n",
    "        b (ndarray): The bias value, added to the weighted sum.\n",
    "        size (int): The layer size/number of neurons.\n",
    "        activation_fn (callable): The neurons' activation function.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, layer_size, activation_fn, d_activation_fn=None):\n",
    "        super().__init__()\n",
    "        # Randomly initializing the parameters (using a normal distribution this time):\n",
    "        self.W = np.random.standard_normal((num_inputs, layer_size))#Size of matrix is num_x of rows and num_layer of rows \n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "        \n",
    "        self.activation_fn = activation_fn\n",
    "        self.d_activation_fn = d_activation_fn # Deriv. activation function\n",
    "        self.x, self.y, self.dL_dW, self.dL_db = 0, 0, 0, 0 # Storage attr.\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward the input signal through the layer.\"\"\"\n",
    "        z = np.dot(x, self.W) + self.b\n",
    "        self.y = self.activation_fn(z)\n",
    "        self.x = x # we store values for back-propagation\n",
    "        return self.y #Output will be a verctor\n",
    "    \n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"Back-propagate the loss.\"\"\"\n",
    "        dy_dz = self.d_activation_fn(self.y)  # = f'\n",
    "        dL_dz = (dL_dy * dy_dz) # dL/dz = dL/dy * dy/dz = l'_{k+1} * f'\n",
    "        dz_dw = self.x.T\n",
    "        dz_dx = self.W.T\n",
    "        dz_db = np.ones(dL_dy.shape[0]) # dz/db = d(W.x + b)/db = 0 + db/db = \"ones\"-vector\n",
    "\n",
    "        # Computing the derivatives with respect to the layer's parameters, and storing them for opt. optimization:\n",
    "        self.dL_dW = np.dot(dz_dw, dL_dz)\n",
    "        self.dL_db = np.dot(dz_db, dL_dz)\n",
    "\n",
    "        # Computing the derivative with respect to the input, to be passed to the previous layers (their `dL_dy`):\n",
    "        dL_dx = np.dot(dL_dz, dz_dx)\n",
    "        return dL_dx\n",
    "    \n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"Optimize the layer's parameters w.r.t. the derivative values.\"\"\"\n",
    "        self.W -= epsilon * self.dL_dW\n",
    "        self.b -= epsilon * self.dL_db\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.25091976  0.90142861]]\n",
      "\n",
      "[[0.46398788 0.19731697]]\n",
      "\n",
      "[[0.28712364 0.         0.33478571]]\n",
      "\n",
      "[[0.         0.         1.08175419]]\n",
      "\n",
      "[[-0.25091976  0.90142861]\n",
      " [ 0.46398788  0.19731697]]\n",
      "\n",
      "[[0.28712364 0.         0.33478571]\n",
      " [0.         0.         1.08175419]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# Random input column-vectors of 2 values (shape = `(1, 2)`):\n",
    "x1 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n",
    "_(x1)\n",
    "x2 = np.random.uniform(-1, 1, 2).reshape(1, 2)\n",
    "_(x2)\n",
    "\n",
    "relu_fn = lambda y: np.maximum(y, 0)    #Defining our activation function\n",
    "layer = FullyConnectedLayer(2, 3, relu_fn)\n",
    "\n",
    "# Our layer can process x1 and x2 separately...\n",
    "out1 = layer.forward(x1)\n",
    "_(out1)\n",
    "\n",
    "out2 = layer.forward(x2)\n",
    "_(out2)\n",
    "\n",
    "#And together:\n",
    "x12 = np.concatenate((x1, x2))# stack of input vectors, of shape `(2, 2)`\n",
    "_(x12)\n",
    "out12 = layer.forward(x12)\n",
    "_(out12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the training and testing data:\n",
    "X_train, y_train = mnist.train_images(), mnist.train_labels()\n",
    "X_test, y_test = mnist.test_images(), mnist.train_labels()\n",
    "num_classes = 10    # classes are digits from 0 to 9\n",
    "\n",
    "# We transform the images into column vectors(as inputs for our NN):\n",
    "X_train, X_test = X_train.reshape(-1, 28*28), X_test.reshape(-1, 28*28)\n",
    "# We \"one-hot\" the labels (as targets for our NN), for instance, transform label `4` into vector `[0, 0, 0, 0, 1, 0, 0, 0, 0, 0]`:\n",
    "y_train = np.eye(num_classes)[y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x): # Apply the sigmoid function to the elements of x.\n",
    "    return 1 / (1 + np.exp(-x)) # y\n",
    "\n",
    "def derivated_sigmoid(y):   # sigmoid derivative function\n",
    "    return y * (1 - y)\n",
    "\n",
    "def loss_L2(pred, target):  # L2 loss function\n",
    "    return np.sum(np.square(pred - target)) / pred.shape[0] # opt. for results not depending on the batch size (pred.shape[0]) , we divide the loss by it\n",
    "\n",
    "def derivated_loss_L2(pred, target):   # L2 derivative function\n",
    "    return 2 * (pred - target) # we could add the batch size division here too, but it wouldn't really affect the training (just scaling down the derivatives).\n",
    "\n",
    "def binary_cross_entropy(pred, target):            # cross-entropy loss function\n",
    "    return -np.mean(np.multiply(np.log(pred), target) + np.multiply(np.log(1 - pred), (1 - target)))\n",
    "\n",
    "\n",
    "def derivated_binary_cross_entropy(pred, target):  # cross-entropy derivative function\n",
    "    return (pred - target) / (pred * (1 - pred))\n",
    "\n",
    "class SimpleNetwork(object):\n",
    "    \"\"\"A simple fully-connected NN.\n",
    "    Args:\n",
    "        num_inputs (int): The input vector size / number of input values. results not depending on the batch size (pred.shape[0]), we divide the loss by it\n",
    "        num_outputs (int): The output vector size.\n",
    "        hidden_layers_sizes (list): A list of sizes for each hidden layer to be added to the network\n",
    "    Attributes:\n",
    "        layers (list): The list of layers forming this simple network.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64,32),\n",
    "                 loss_fn = loss_L2, d_loss_fn=derivated_loss_L2):\n",
    "        super().__init__()\n",
    "        # We build the list of layers composing the network:\n",
    "        sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n",
    "        self.layers = [\n",
    "            FullyConnectedLayer(sizes[i], sizes[i+1], sigmoid, derivated_sigmoid)\n",
    "            for i in range(len(sizes) -1)]\n",
    "        self.loss_fn, self.d_loss_fn = loss_fn, d_loss_fn\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward the input vector `x` through the layers.\"\"\"\n",
    "        for layer in self.layers: #From the input layer to the output one\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"Compute the output corresponding to `x`, and return the index of the largest output value\"\"\"\n",
    "        estimations = self.forward(x)\n",
    "        best_class = np.argmax(estimations)\n",
    "        return best_class\n",
    "    \n",
    "    def evaluate_accuracy(self, X_val, y_val):\n",
    "        \"\"\"Evaluate the network's accuracy on a validate dataset.\"\"\"\n",
    "        num_corrects = 0\n",
    "        for i in range(len(X_val)):\n",
    "            if self.predict(X_val[i]) == y_val[i]:\n",
    "                num_corrects += 1\n",
    "        return num_corrects / len(X_val)\n",
    "    \n",
    "    def backward(self, dL_dy):\n",
    "        \"\"\"Back-propagate the loss derivative from last to 1st layer.\"\"\"\n",
    "        for layer in reversed(self.layers):\n",
    "            dL_dy = layer.backward(dL_dy)\n",
    "        return dL_dy\n",
    "    \n",
    "    def optimize(self, epsilon):\n",
    "        \"\"\"Optimize the parameters according to the stored gradients.\"\"\"\n",
    "        for layer in self.layers:\n",
    "            layer.optimize(epsilon)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val, y_val, batch_size=32, num_epochs=5, learning_rate=5e-3):\n",
    "        \"\"\"Train (and evaluate) the network on the provided dataset.\"\"\"\n",
    "        num_batches_per_epoch = len(X_train) // batch_size\n",
    "        loss, accuracy = [], []\n",
    "        for i in range(num_epochs) : # for each training epoch\n",
    "            epoch_loss = 0\n",
    "            for b in range(num_batches_per_epoch): # for each batch\n",
    "                # Get batch:\n",
    "                b_idx = b * batch_size\n",
    "                b_idx_e = b_idx + batch_size\n",
    "                b_idx_e = b_idx + batch_size\n",
    "                x, y_true = X_train[b_idx:b_idx_e], y_train[b_idx:b_idx_e]\n",
    "                # Optimixe on batch:\n",
    "                y = self.forward(x) # forward pass\n",
    "                epoch_loss += self.loss_fn(y, y_true) # LOSS\n",
    "                dL_dy = self.d_loss_fn(y, y_true) # loss derivation\n",
    "                self.backward(dL_dy) # back-propogation pass\n",
    "                self.optimize(learning_rate) # optimization\n",
    "                \n",
    "            loss.append(epoch_loss / num_batches_per_epoch)\n",
    "            # After each epoch, we \"validate\" our network, i.e., we measure its accuracy over the test/validation set:\n",
    "            accuracy.append(self.evaluate_accuracy(X_val, y_val))\n",
    "            print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(i, loss[i], accuracy[i] * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.train_images(), mnist.train_labels()\n",
    "X_test,  y_test  = mnist.test_images(), mnist.test_labels()\n",
    "num_classes = 10    # classes are the digits from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(-1, 28 * 28), X_test.reshape(-1, 28 * 28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized pixel values between 0.0 and 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = X_train / 255., X_test / 255.\n",
    "print(\"Normalized pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.eye(num_classes)[y_train]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mnist_classifier = SimpleNetwork(num_inputs=X_train.shape[1], \n",
    "                                 num_outputs=num_classes, hidden_layers_sizes=[64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained : training loss = 3.654294 | val accuracy = 10.54%\n"
     ]
    }
   ],
   "source": [
    "predictions = mnist_classifier.forward(X_train)                         # forward pass\n",
    "loss_untrained = mnist_classifier.loss_fn(predictions, y_train)   # loss computation\n",
    "\n",
    "accuracy_untrained = mnist_classifier.evaluate_accuracy(X_test, y_test)  # Accuracy\n",
    "print(\"Untrained : training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "    loss_untrained, accuracy_untrained * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: training loss = 0.739994 | val accuracy = 61.10%\n",
      "Epoch    1: training loss = 0.442242 | val accuracy = 76.49%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-fbdec2441609>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m losses, accuracies = mnist_classifier.train(X_train, y_train, X_test, y_test, \n\u001b[1;32m----> 2\u001b[1;33m                                             batch_size=30, num_epochs=500)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-647f9363acff>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, X_val, y_val, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     83\u001b[0m                 \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb_idx_e\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mb_idx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mb_idx_e\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m                 \u001b[1;31m# Optimixe on batch:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# forward pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# LOSS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mdL_dy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_loss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# loss derivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-647f9363acff>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[1;34m\"\"\"Forward the input vector `x` through the layers.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#From the input layer to the output one\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-56c9b0b09281>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;34m\"\"\"Forward the input signal through the layer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;31m# we store values for back-propagation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses, accuracies = mnist_classifier.train(X_train, y_train, X_test, y_test, \n",
    "                                            batch_size=30, num_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1, y_train1 = mnist.train_images(), mnist.train_labels()\n",
    "X_test1,  y_test1  = mnist.test_images(), mnist.test_labels()\n",
    "num_classes = 10    # classes are the digits from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = (y_train1 == 5).astype(np.int)\n",
    "y_test_binary = (y_test1 == 5).astype(np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_binary = np.eye(2)[y_train_binary]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_classifier_binary = SimpleNetwork(num_inputs=X_train.shape[1], \n",
    "                                 num_outputs=2, hidden_layers_sizes=[784, 512, 256, 128, 64, 32])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained : training loss = 0.708148 | val accuracy = 9.08%\n"
     ]
    }
   ],
   "source": [
    "predictions1 = mnist_classifier_binary.forward(X_train)                         # forward pass\n",
    "loss_untrained1 = mnist_classifier_binary.loss_fn(predictions1, y_train_binary)   # loss computation\n",
    "\n",
    "accuracy_untrained1 = mnist_classifier_binary.evaluate_accuracy(X_test, y_test)  # Accuracy\n",
    "print(\"Untrained : training loss = {:.6f} | val accuracy = {:.2f}%\".format(\n",
    "    loss_untrained1, accuracy_untrained1 * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0: training loss = 0.163492 | val accuracy = 91.08%\n",
      "Epoch    1: training loss = 0.128874 | val accuracy = 93.83%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-541ae6cd85ce>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m losses1, accuracies1 = mnist_classifier_binary.train(X_train, y_train_binary, X_test, y_test_binary, \n\u001b[1;32m----> 2\u001b[1;33m                                             batch_size=30, num_epochs=500)\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-647f9363acff>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, X_val, y_val, batch_size, num_epochs, learning_rate)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# LOSS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[0mdL_dy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0md_loss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# loss derivation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdL_dy\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# back-propogation pass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# optimization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-647f9363acff>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dL_dy)\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;34m\"\"\"Back-propagate the loss derivative from last to 1st layer.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m             \u001b[0mdL_dy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdL_dy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdL_dy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-5-56c9b0b09281>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dL_dy)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[1;31m# Computing the derivatives with respect to the layer's parameters, and storing them for opt. optimization:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdL_dW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz_dw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdL_dz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdL_db\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdz_db\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdL_dz\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "losses1, accuracies1 = mnist_classifier_binary.train(X_train, y_train_binary, X_test, y_test_binary, \n",
    "                                            batch_size=30, num_epochs=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
